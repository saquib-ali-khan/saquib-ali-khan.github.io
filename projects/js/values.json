{"version":"1.0","encoding":"UTF-8","feed":{"xmlns":"http://www.w3.org/2005/Atom","xmlns$openSearch":"http://a9.com/-/spec/opensearchrss/1.0/","xmlns$gsx":"http://schemas.google.com/spreadsheets/2006/extended","id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Sheet1"},"link":[{"rel":"alternate","type":"application/atom+xml","href":"https://docs.google.com/spreadsheets/d/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/pubhtml"},{"rel":"http://schemas.google.com/g/2005#feed","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values"},{"rel":"http://schemas.google.com/g/2005#post","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values"},{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values?alt\u003djson"}],"author":[{"name":{"$t":"sumit.binnani"},"email":{"$t":"sumit.binnani@gmail.com"}}],"openSearch$totalResults":{"$t":"29"},"openSearch$startIndex":{"$t":"1"},"entry":[{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/cokwr"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Predicting_Building_Abandonment"},"content":{"type":"text","$t":"projecttitle: Predicting Building Abandonment, tags: Machine Learning, sckit-learn, pandas, Python, desc: The project involved predicting building abandonment (\u0026ldquo;blight\u0026rdquo;) based on several public datasets including criminal incidents and permit violation. The datasets include latitude and longitude, which have been grouped together for identification of building. The grouping was accomplished by using \u003cem\u003eGoogle Reverse Geo-Coding API\u003c/em\u003e, however considering millions of records to be fetched, the building had been approximated as 7-char \u003cem\u003egeohash\u003c/em\u003e corresponding to the given latitude-longitude pair of incidents. The labeled dataset for training had been generated using blight-incidents dataset having incident marked as \u0026lsquo;Dismantle\u0026rsquo;. The code and report for the same can be found in following GitHub repository., link: https://github.com/sumitbinnani/DataScienceAtScaleCapstoneProject"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/cokwr"}],"gsx$id":{"$t":"Predicting_Building_Abandonment"},"gsx$projecttitle":{"$t":"Predicting Building Abandonment"},"gsx$tags":{"$t":"Machine Learning, sckit-learn, pandas, Python"},"gsx$desc":{"$t":"The project involved predicting building abandonment (\u0026ldquo;blight\u0026rdquo;) based on several public datasets including criminal incidents and permit violation. The datasets include latitude and longitude, which have been grouped together for identification of building. The grouping was accomplished by using \u003cem\u003eGoogle Reverse Geo-Coding API\u003c/em\u003e, however considering millions of records to be fetched, the building had been approximated as 7-char \u003cem\u003egeohash\u003c/em\u003e corresponding to the given latitude-longitude pair of incidents. The labeled dataset for training had been generated using blight-incidents dataset having incident marked as \u0026lsquo;Dismantle\u0026rsquo;. The code and report for the same can be found in following GitHub repository."},"gsx$link":{"$t":"https://github.com/sumitbinnani/DataScienceAtScaleCapstoneProject"}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/cpzh4"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Predict__Predicting_Users_for_Advertising_"},"content":{"type":"text","$t":"projecttitle: Predict (Predicting Users for Advertising), tags: Machine Learning, Big Data, Naïve Bayes, Random Forest, K-Means Clustering, Behavioral Targeting, Hive, Spark MLlib, Spark Streaming, Kafka, desc: The project aims at predicting users who would take desirable action after an advertisement is displayed to them, and target them in real time. The current model is powered by \u003cem\u003eNaives Bayes Classifier\u003c/em\u003e built by aggregating user\u0026rsquo;s browsing behavior and geolocation. We are in the process of migrating the model to \u003cem\u003eRandom Forest\u003c/em\u003e which has provided about 10% uplift in the preliminary analysis. To combat memory issues stemming out of the large feature set while using \u003cem\u003eRandom Forest\u003c/em\u003e, we grouped the features by \u003cem\u003eK-Means\u003c/em\u003e \u003cem\u003eClustering\u003c/em\u003e and used aggregated count of the group to train the model. Moving forward we would be incorporating temporal behavior in the model\u0026rsquo;s feature set, and move to real-time prediction using Spark Streaming."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/cpzh4"}],"gsx$id":{"$t":"Predict__Predicting_Users_for_Advertising_"},"gsx$projecttitle":{"$t":"Predict (Predicting Users for Advertising)"},"gsx$tags":{"$t":"Machine Learning, Big Data, Naïve Bayes, Random Forest, K-Means Clustering, Behavioral Targeting, Hive, Spark MLlib, Spark Streaming, Kafka"},"gsx$desc":{"$t":"The project aims at predicting users who would take desirable action after an advertisement is displayed to them, and target them in real time. The current model is powered by \u003cem\u003eNaives Bayes Classifier\u003c/em\u003e built by aggregating user\u0026rsquo;s browsing behavior and geolocation. We are in the process of migrating the model to \u003cem\u003eRandom Forest\u003c/em\u003e which has provided about 10% uplift in the preliminary analysis. To combat memory issues stemming out of the large feature set while using \u003cem\u003eRandom Forest\u003c/em\u003e, we grouped the features by \u003cem\u003eK-Means\u003c/em\u003e \u003cem\u003eClustering\u003c/em\u003e and used aggregated count of the group to train the model. Moving forward we would be incorporating temporal behavior in the model\u0026rsquo;s feature set, and move to real-time prediction using Spark Streaming."},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/cre1l"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Predictive_Client_side_Profiling_for_Behavioral_Advertising"},"content":{"type":"text","$t":"projecttitle: Predictive Client-side Profiling for Behavioral Advertising, tags: Real-time Prediction, Machine Learning, Client-side Profiling, Behavioral Targeting, Ad-exchange Architecture, JavaScript, HTML, Python, Kafka, NLP, desc: The idea was harnessed as part of a Hackathon at Media IQ Digital India Ltd. Leveraging know-how of Web Development and Data Science, I proposed a novel scalable approach for client-side user profiling to offer personalized advertising experience. The proposed methodology\u0026rsquo;s merit includes near-real-time user profiling, high scalability, low-bandwidth requirement, privacy issues redressal, and minimal changes to existing ad-exchange architecture. The scalability and privacy issue are addressed by avoiding server-side data aggregation, which in turn also reduces server\u0026rsquo;s storage and computational requirements. The approach can be easily integrated with existing ad-exchange platforms, and bid optimization frameworks, allowing advertisers highly-granular ad personalization without significant infrastructure investments. \u003cstrong\u003eThe methodology is currently under scrutiny for patent application\u003c/strong\u003e."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/cre1l"}],"gsx$id":{"$t":"Predictive_Client_side_Profiling_for_Behavioral_Advertising"},"gsx$projecttitle":{"$t":"Predictive Client-side Profiling for Behavioral Advertising"},"gsx$tags":{"$t":"Real-time Prediction, Machine Learning, Client-side Profiling, Behavioral Targeting, Ad-exchange Architecture, JavaScript, HTML, Python, Kafka, NLP"},"gsx$desc":{"$t":"The idea was harnessed as part of a Hackathon at Media IQ Digital India Ltd. Leveraging know-how of Web Development and Data Science, I proposed a novel scalable approach for client-side user profiling to offer personalized advertising experience. The proposed methodology\u0026rsquo;s merit includes near-real-time user profiling, high scalability, low-bandwidth requirement, privacy issues redressal, and minimal changes to existing ad-exchange architecture. The scalability and privacy issue are addressed by avoiding server-side data aggregation, which in turn also reduces server\u0026rsquo;s storage and computational requirements. The approach can be easily integrated with existing ad-exchange platforms, and bid optimization frameworks, allowing advertisers highly-granular ad personalization without significant infrastructure investments. \u003cstrong\u003eThe methodology is currently under scrutiny for patent application\u003c/strong\u003e."},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/chk2m"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Audience_Pooling_Framework"},"content":{"type":"text","$t":"projecttitle: Audience Pooling Framework, tags: Map-Reduce, Hadoop Streaming, Data Structures, Design Pattern, Scalable Search Platform, Big Data, Haddop Streaming, Java, desc: The aim of the project was to develop query engine to filter data from daily feed satisfying any given combination of rules. Since the aim was fast retrieval of records satisfying predefined conditions, I implemented data structures, which despite higher insertion and deletion cost, were fast when it came to retrieval. Storing this data structure in memory of individual mappers, and using Hadoop streaming API, the platform is capable of handling complex queries. Also, the current code can be easily scaled to define new rules owing to the modular code which is built upon design patterns like \u003cem\u003eFactory Pattern\u003c/em\u003e, \u003cem\u003eCommand Pattern\u003c/em\u003e, and \u003cem\u003eChain of Responsibility\u003c/em\u003e. For the ease of usage, I also added an UI on top of the platform for configuring rules. The platform now powers insights which previously were quite difficult to obtain."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/chk2m"}],"gsx$id":{"$t":"Audience_Pooling_Framework"},"gsx$projecttitle":{"$t":"Audience Pooling Framework"},"gsx$tags":{"$t":"Map-Reduce, Hadoop Streaming, Data Structures, Design Pattern, Scalable Search Platform, Big Data, Haddop Streaming, Java"},"gsx$desc":{"$t":"The aim of the project was to develop query engine to filter data from daily feed satisfying any given combination of rules. Since the aim was fast retrieval of records satisfying predefined conditions, I implemented data structures, which despite higher insertion and deletion cost, were fast when it came to retrieval. Storing this data structure in memory of individual mappers, and using Hadoop streaming API, the platform is capable of handling complex queries. Also, the current code can be easily scaled to define new rules owing to the modular code which is built upon design patterns like \u003cem\u003eFactory Pattern\u003c/em\u003e, \u003cem\u003eCommand Pattern\u003c/em\u003e, and \u003cem\u003eChain of Responsibility\u003c/em\u003e. For the ease of usage, I also added an UI on top of the platform for configuring rules. The platform now powers insights which previously were quite difficult to obtain."},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/ciyn3"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"AWS_Bot"},"content":{"type":"text","$t":"projecttitle: AWS Bot, tags: Notification Bot, Productivity Tool, NodeJs, desc: This is a Slack Bot which is used for monitoring AWS EMR Clusters spawned at the company, and provide notification in case the cluster is idle. It also automatically terminates clusters which had been idle for more than 2 hours. The bot had been paramount in cost saving with monthly saving greater than USD 1k monthly"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/ciyn3"}],"gsx$id":{"$t":"AWS_Bot"},"gsx$projecttitle":{"$t":"AWS Bot"},"gsx$tags":{"$t":"Notification Bot, Productivity Tool, NodeJs"},"gsx$desc":{"$t":"This is a Slack Bot which is used for monitoring AWS EMR Clusters spawned at the company, and provide notification in case the cluster is idle. It also automatically terminates clusters which had been idle for more than 2 hours. The bot had been paramount in cost saving with monthly saving greater than USD 1k monthly"},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/ckd7g"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Deep_Learning_for_Parking_Slot_Occupancy"},"content":{"type":"text","$t":"projecttitle: Deep Learning for Parking Slot Occupancy, tags: Deep Learning, Computer Vision, Transfer Learning, Tensorflow, Keras, Python, desc: The aim of the project was to develop a vision based model for identification of empty parking slot. I used PKLot dataset (Almeida, P., Oliveira, L. S., Silva Jr, E., Britto Jr, A., Koerich, A., PKLot - A robust dataset for parking lot classification, Expert Systems with Applications, 42(11):4937-4949, 2015) for training the model. As preprocessing I resized all the images to 32x32 before proceeding with the training. The model was designed as multinomial classification and was built upon Google\u0026rsquo;s Inception Model. By using Transfer Learning Technique, I achieved a classification accuracy of 96% on the balanced dataset. Further, randomly sampling images with a replacement for generation of training batches gave better results as compared to predefined batches, and boosted the accuracy to 98%."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/ckd7g"}],"gsx$id":{"$t":"Deep_Learning_for_Parking_Slot_Occupancy"},"gsx$projecttitle":{"$t":"Deep Learning for Parking Slot Occupancy"},"gsx$tags":{"$t":"Deep Learning, Computer Vision, Transfer Learning, Tensorflow, Keras, Python"},"gsx$desc":{"$t":"The aim of the project was to develop a vision based model for identification of empty parking slot. I used PKLot dataset (Almeida, P., Oliveira, L. S., Silva Jr, E., Britto Jr, A., Koerich, A., PKLot - A robust dataset for parking lot classification, Expert Systems with Applications, 42(11):4937-4949, 2015) for training the model. As preprocessing I resized all the images to 32x32 before proceeding with the training. The model was designed as multinomial classification and was built upon Google\u0026rsquo;s Inception Model. By using Transfer Learning Technique, I achieved a classification accuracy of 96% on the balanced dataset. Further, randomly sampling images with a replacement for generation of training batches gave better results as compared to predefined batches, and boosted the accuracy to 98%."},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/clrrx"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Analyzing_Social_Network"},"content":{"type":"text","$t":"projecttitle: Analyzing Social Network, tags: Visualization, Mathematical Modelling, Social Network, Graphs, WebCrawler, Python, Selenium, PhantomJS, d3.j, desc: The aim of the project was to visualize my social network, and find clusters on basis of connections. To accomplished the task, I wrote a python based web crawler using PhantomJS and Selenium to crawl friends\u0026rsquo; list on Facebook (as Graph API restricts a number of results substantially). The data so obtained was then used to draw a graph by modeling people as charges and connection between them as spring. The modeling of charge and spring brought together the clusters which were closely connected and thus provided clusters without any other attributes. The clusters obtained once the people (charges) are similar despite their random initialization. Closely looking at the clusters, we find that there are clusters within clusters. Also, the people lying between two clusters are usually common to both of the clusters., link: https://sumitbinnani.github.io/projects/analyzing-facebook-social-network.html"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/clrrx"}],"gsx$id":{"$t":"Analyzing_Social_Network"},"gsx$projecttitle":{"$t":"Analyzing Social Network"},"gsx$tags":{"$t":"Visualization, Mathematical Modelling, Social Network, Graphs, WebCrawler, Python, Selenium, PhantomJS, d3.j"},"gsx$desc":{"$t":"The aim of the project was to visualize my social network, and find clusters on basis of connections. To accomplished the task, I wrote a python based web crawler using PhantomJS and Selenium to crawl friends\u0026rsquo; list on Facebook (as Graph API restricts a number of results substantially). The data so obtained was then used to draw a graph by modeling people as charges and connection between them as spring. The modeling of charge and spring brought together the clusters which were closely connected and thus provided clusters without any other attributes. The clusters obtained once the people (charges) are similar despite their random initialization. Closely looking at the clusters, we find that there are clusters within clusters. Also, the people lying between two clusters are usually common to both of the clusters."},"gsx$link":{"$t":"https://sumitbinnani.github.io/projects/analyzing-facebook-social-network.html"}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/cyevm"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Pupil_Detection_for_Gaze_Tracking"},"content":{"type":"text","$t":"projecttitle: Pupil Detection for Gaze Tracking, tags: Pupil Detection, Support Vector Machines, Kalman Filter, Python, OpenCV, scikit-learn, pandas, desc: The aim of the project was to generate heatmap by tracking user\u0026rsquo;s gaze on screen. We implemented gaze tracking system by implementing pupil capturing using eye center localization as proposed by Timm F. and Erhardt B. in their paper \u003cem\u003eAccurate Eye Centre Localisation by Means of Gradients\u003c/em\u003e \u003cem\u003eVISAPP 11 (2011)\u003c/em\u003e and later trained SVM model to track user\u0026rsquo;s gaze on screen. The output by SVM model was wavy and to get a smoother distribution, we later added unimodal probability distribution obtained by Kalman Filter. The module developed during the hackathon was later taken for integration by UX Researchers at Oracle to perform visual analysis of attention span for web content."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/cyevm"}],"gsx$id":{"$t":"Pupil_Detection_for_Gaze_Tracking"},"gsx$projecttitle":{"$t":"Pupil Detection for Gaze Tracking"},"gsx$tags":{"$t":"Pupil Detection, Support Vector Machines, Kalman Filter, Python, OpenCV, scikit-learn, pandas"},"gsx$desc":{"$t":"The aim of the project was to generate heatmap by tracking user\u0026rsquo;s gaze on screen. We implemented gaze tracking system by implementing pupil capturing using eye center localization as proposed by Timm F. and Erhardt B. in their paper \u003cem\u003eAccurate Eye Centre Localisation by Means of Gradients\u003c/em\u003e \u003cem\u003eVISAPP 11 (2011)\u003c/em\u003e and later trained SVM model to track user\u0026rsquo;s gaze on screen. The output by SVM model was wavy and to get a smoother distribution, we later added unimodal probability distribution obtained by Kalman Filter. The module developed during the hackathon was later taken for integration by UX Researchers at Oracle to perform visual analysis of attention span for web content."},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/cztg3"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Geocleansing_of_Location_Data"},"content":{"type":"text","$t":"projecttitle: Geocleansing of Location Data, tags: Natural Language Processing, Phonetic Algorithm, Soundex, Fuzzy Logic, PLSQL, desc: The project aimed at cleansing massive geographical datasets to get rid of typographical errors and duplicate records. I initially found similar records by using \u003cem\u003eLevenshtein Distance\u003c/em\u003e as similarity matrix, however, this led to a high number of false positives (about 30% of predicted duplicates). To tighten the false positives, I later implemented fuzzy logic using \u003cem\u003eLevenshtein Distance\u003c/em\u003e, \u003cem\u003eSoundex\u003c/em\u003e, and \u003cem\u003eNew York State Identification and Intelligence System\u003c/em\u003e. The accuracy of the model was further enhanced by imposing hierarchy constraints on locations (e.g. only cities in same states would be considered for matching, and the states should have been already cleaned before proceeding with cities). The precision for the tool was \u003cstrong\u003e88%\u003c/strong\u003e, with a miss of \u003cstrong\u003e4%\u003c/strong\u003e."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/cztg3"}],"gsx$id":{"$t":"Geocleansing_of_Location_Data"},"gsx$projecttitle":{"$t":"Geocleansing of Location Data"},"gsx$tags":{"$t":"Natural Language Processing, Phonetic Algorithm, Soundex, Fuzzy Logic, PLSQL"},"gsx$desc":{"$t":"The project aimed at cleansing massive geographical datasets to get rid of typographical errors and duplicate records. I initially found similar records by using \u003cem\u003eLevenshtein Distance\u003c/em\u003e as similarity matrix, however, this led to a high number of false positives (about 30% of predicted duplicates). To tighten the false positives, I later implemented fuzzy logic using \u003cem\u003eLevenshtein Distance\u003c/em\u003e, \u003cem\u003eSoundex\u003c/em\u003e, and \u003cem\u003eNew York State Identification and Intelligence System\u003c/em\u003e. The accuracy of the model was further enhanced by imposing hierarchy constraints on locations (e.g. only cities in same states would be considered for matching, and the states should have been already cleaned before proceeding with cities). The precision for the tool was \u003cstrong\u003e88%\u003c/strong\u003e, with a miss of \u003cstrong\u003e4%\u003c/strong\u003e."},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/d180g"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Extraction_of_Biomedical_Implant"},"content":{"type":"text","$t":"projecttitle: Extraction of Biomedical Implant, tags: Biomedical Imaging, Image Processing, Pattern Matching, Morphological Transformations, MATLAB, OpenCV, Android SDK, desc: At GlobalLogic, I worked for one of the top-3 medical technological conglomerates in the world, and worked on the development of image processing pipeline for extraction of the biomedical implant from the x-ray. The task was preceded by denoising image using a median filter, and improving contrast in an image by applying adaptive histogram equalization. The denoised-contrast-enhanced image was then passed to morphological filters to find possible candidates for the implant, and finally, the candidates were narrowed down using pattern matching (on image descriptors). The final decision still to select the position of implant still lied with the operator. The prototyping of the approach was done using MATLAB, and the code was later ported to Android."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/d180g"}],"gsx$id":{"$t":"Extraction_of_Biomedical_Implant"},"gsx$projecttitle":{"$t":"Extraction of Biomedical Implant"},"gsx$tags":{"$t":"Biomedical Imaging, Image Processing, Pattern Matching, Morphological Transformations, MATLAB, OpenCV, Android SDK"},"gsx$desc":{"$t":"At GlobalLogic, I worked for one of the top-3 medical technological conglomerates in the world, and worked on the development of image processing pipeline for extraction of the biomedical implant from the x-ray. The task was preceded by denoising image using a median filter, and improving contrast in an image by applying adaptive histogram equalization. The denoised-contrast-enhanced image was then passed to morphological filters to find possible candidates for the implant, and finally, the candidates were narrowed down using pattern matching (on image descriptors). The final decision still to select the position of implant still lied with the operator. The prototyping of the approach was done using MATLAB, and the code was later ported to Android."},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/d2mkx"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Electronics_Aid_for_elder_and_Sick"},"content":{"type":"text","$t":"projecttitle: Electronics Aid for elder and Sick, tags: Embedded System, Assistive Aid, Microcontroller Programming, Wireless Switches, IR Reciever, C, Arduino, Sensor Interfacing, desc: The project started as a development of \u003cem\u003eSmart Home \u003c/em\u003emodule, and later got focused on the development of wireless switches as an aid for elderly and sick people. The choice of infra-red for wireless communication was to avoid interference with other medical devices in hospitals. The task was accomplished by using decoding IR Signal from TV-remote controller and triggering relays to operate electrical appliances. Since the frequency at which the IR Signals are emitted from the remote controller is high, the input from IR receiver was read using a lower level of abstractions. The control signals from the remote were easy to identify as the remote controller had a distinctive start and end sequence. The work done was published in\u003cem\u003e IEEE Conference Proceeding of Recent Advances and Innovations in Engineering (ICRAIE), 2014\u003c/em\u003e. The project was further enhanced by integrating it with a web layer and establishing a connection between Arduino and server using serial communication."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/d2mkx"}],"gsx$id":{"$t":"Electronics_Aid_for_elder_and_Sick"},"gsx$projecttitle":{"$t":"Electronics Aid for elder and Sick"},"gsx$tags":{"$t":"Embedded System, Assistive Aid, Microcontroller Programming, Wireless Switches, IR Reciever, C, Arduino, Sensor Interfacing"},"gsx$desc":{"$t":"The project started as a development of \u003cem\u003eSmart Home \u003c/em\u003emodule, and later got focused on the development of wireless switches as an aid for elderly and sick people. The choice of infra-red for wireless communication was to avoid interference with other medical devices in hospitals. The task was accomplished by using decoding IR Signal from TV-remote controller and triggering relays to operate electrical appliances. Since the frequency at which the IR Signals are emitted from the remote controller is high, the input from IR receiver was read using a lower level of abstractions. The control signals from the remote were easy to identify as the remote controller had a distinctive start and end sequence. The work done was published in\u003cem\u003e IEEE Conference Proceeding of Recent Advances and Innovations in Engineering (ICRAIE), 2014\u003c/em\u003e. The project was further enhanced by integrating it with a web layer and establishing a connection between Arduino and server using serial communication."},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/cssly"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Realtime_Reactor_Control_Simulator"},"content":{"type":"text","$t":"projecttitle: Realtime Reactor Control Simulator, tags: Numerical Techniques, Euler’s Method, Ruge-Kutta Method, Control Systems, MATLAB, LabVIEW, NI PCI-6031e, desc: The project aimed at stimulating dynamics of a nuclear reactor in real-time. To simulate the behavior of reactor, we solved \u003cem\u003ePoint Kinetic Equation\u003c/em\u003e using \u003cem\u003eRunge-Kutta Method\u003c/em\u003e (which was selected after benchmarking it against other numerical techniques like \u003cem\u003eEuler Method\u003c/em\u003e and \u003cem\u003eNewton-Rhapson Method\u003c/em\u003e with an aim to optimize speed for bigger step sizes). We then designed feedback system by formulating the relationship between error and gain value, obtained by plotting optimum gain values vs. equilibrium power, and thus, empowered the system to take a decision based on a mathematical model. The system UI was built using LabVIEW and the data acquisition was done using NI PCI-6031e."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/cssly"}],"gsx$id":{"$t":"Realtime_Reactor_Control_Simulator"},"gsx$projecttitle":{"$t":"Realtime Reactor Control Simulator"},"gsx$tags":{"$t":"Numerical Techniques, Euler’s Method, Ruge-Kutta Method, Control Systems, MATLAB, LabVIEW, NI PCI-6031e"},"gsx$desc":{"$t":"The project aimed at stimulating dynamics of a nuclear reactor in real-time. To simulate the behavior of reactor, we solved \u003cem\u003ePoint Kinetic Equation\u003c/em\u003e using \u003cem\u003eRunge-Kutta Method\u003c/em\u003e (which was selected after benchmarking it against other numerical techniques like \u003cem\u003eEuler Method\u003c/em\u003e and \u003cem\u003eNewton-Rhapson Method\u003c/em\u003e with an aim to optimize speed for bigger step sizes). We then designed feedback system by formulating the relationship between error and gain value, obtained by plotting optimum gain values vs. equilibrium power, and thus, empowered the system to take a decision based on a mathematical model. The system UI was built using LabVIEW and the data acquisition was done using NI PCI-6031e."},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/cu76f"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Billing_and_Inventory_Management_System"},"content":{"type":"text","$t":"projecttitle: Billing and Inventory Management System, tags: Software System, Database Management, Credit Management, C#, MySQL, desc: The project aimed at developing a credit-based billing system for the student, and inventory management system for messes and canteens in BITS Pilani. The system was designed and developed after multiple interactions with mess workers (many of whom were deprived of educations), to enhance the user experience. The system helped in increasing transparency in the billing system and our team was recognized for the same by SSMS. The system is currently deployed in all the messes and canteens of BITS Pilani."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/cu76f"}],"gsx$id":{"$t":"Billing_and_Inventory_Management_System"},"gsx$projecttitle":{"$t":"Billing and Inventory Management System"},"gsx$tags":{"$t":"Software System, Database Management, Credit Management, C#, MySQL"},"gsx$desc":{"$t":"The project aimed at developing a credit-based billing system for the student, and inventory management system for messes and canteens in BITS Pilani. The system was designed and developed after multiple interactions with mess workers (many of whom were deprived of educations), to enhance the user experience. The system helped in increasing transparency in the billing system and our team was recognized for the same by SSMS. The system is currently deployed in all the messes and canteens of BITS Pilani."},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/cvlqs"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Unit_Commitment__Optimizing_Operation_Schedule_for_Power_Generation"},"content":{"type":"text","$t":"projecttitle: Unit Commitment: Optimizing Operation Schedule for Power Generation, tags: Mathematical Modelling, Unit Commitment, Dynamic Programming, Lagrange’s Relaxation, MATLAB, desc: The project aimed at determining the operation schedule for generating units for varying loads under different constraints and environments. The optimization problem was tackled by transforming it into an unconstrained optimization problem using \u003cem\u003eLagrange\u0026rsquo;s Relaxation\u003c/em\u003e."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/cvlqs"}],"gsx$id":{"$t":"Unit_Commitment__Optimizing_Operation_Schedule_for_Power_Generation"},"gsx$projecttitle":{"$t":"Unit Commitment: Optimizing Operation Schedule for Power Generation"},"gsx$tags":{"$t":"Mathematical Modelling, Unit Commitment, Dynamic Programming, Lagrange’s Relaxation, MATLAB"},"gsx$desc":{"$t":"The project aimed at determining the operation schedule for generating units for varying loads under different constraints and environments. The optimization problem was tackled by transforming it into an unconstrained optimization problem using \u003cem\u003eLagrange\u0026rsquo;s Relaxation\u003c/em\u003e."},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/cx0b9"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Recreation_of_Virtual_3D_World_from_SLAM_Map"},"content":{"type":"text","$t":"projecttitle: Recreation of Virtual 3D-World from SLAM Map, tags: Virtual Reality, Image Processing, Morphological Transformation, OpenCV, Python, desc: The project aimed at a recreation of virtual 3d-world from the SLAM Map obtained using Laser-SLAM. The task was accomplished by denoising the image by the median filter to remove speckles, and Gaussian Blur followed by contour detection. The detected contours were then scaled and used to obtain the position of walls to be recreated in Virtual World."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/cx0b9"}],"gsx$id":{"$t":"Recreation_of_Virtual_3D_World_from_SLAM_Map"},"gsx$projecttitle":{"$t":"Recreation of Virtual 3D-World from SLAM Map"},"gsx$tags":{"$t":"Virtual Reality, Image Processing, Morphological Transformation, OpenCV, Python"},"gsx$desc":{"$t":"The project aimed at a recreation of virtual 3d-world from the SLAM Map obtained using Laser-SLAM. The task was accomplished by denoising the image by the median filter to remove speckles, and Gaussian Blur followed by contour detection. The detected contours were then scaled and used to obtain the position of walls to be recreated in Virtual World."},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/d9ney"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Virtual_Immersive_Cognitive_Exercises"},"content":{"type":"text","$t":"projecttitle: Virtual Immersive Cognitive Exercises, tags: Virtual Reality, Game Engine, Collision Detection, Electro-oculography, C, C++, Oculus Rift, desc: Worked on a head mounted tracking system for spatial capability assessment of humans. As part of the project, we developed Virtual Immersive Cognitive Exercises for Alzheimer and Dementia\u0026rsquo;s patients using Oculus Rift as head mounted gear, and VR chair integrated with a joystick to move around the virtual world. For stabilizing the rendered content, I also worked on electro-oculography to understand eye-movements while walking with Oculus Rift. We also worked on the creation of a few neuro-rehabilitation games for the treatment of people who have visual processing problems."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/d9ney"}],"gsx$id":{"$t":"Virtual_Immersive_Cognitive_Exercises"},"gsx$projecttitle":{"$t":"Virtual Immersive Cognitive Exercises"},"gsx$tags":{"$t":"Virtual Reality, Game Engine, Collision Detection, Electro-oculography, C, C++, Oculus Rift"},"gsx$desc":{"$t":"Worked on a head mounted tracking system for spatial capability assessment of humans. As part of the project, we developed Virtual Immersive Cognitive Exercises for Alzheimer and Dementia\u0026rsquo;s patients using Oculus Rift as head mounted gear, and VR chair integrated with a joystick to move around the virtual world. For stabilizing the rendered content, I also worked on electro-oculography to understand eye-movements while walking with Oculus Rift. We also worked on the creation of a few neuro-rehabilitation games for the treatment of people who have visual processing problems."},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/db1zf"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Histogram_Equalization_for_Facial_Features_Extraction"},"content":{"type":"text","$t":"projecttitle: Histogram Equalization for Facial Features Extraction, tags: Image Processing, Computer Vision, Contrast Enhancement, Adaptive Histogram Equalization, C, MATLAB, Python, OpenCV, desc: The project aimed at developing robust image processing pipeline for enhancing contrast and edges in the facial image as a precursor to facial feature extraction. The task was accomplished by implementing \u003cem\u003eContrast Limited Adaptive Histogram Equalization (CLAHE)\u003c/em\u003e. The image so obtained was noisy and to suppress the same, the CLAHE was followed by applying \u003cem\u003eBidirectional Weighted Median Filter\u003c/em\u003e. The Proof-of-Concept was tested using MATLAB and Python, and the code was later ported to VHDL for FPGA programming."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/db1zf"}],"gsx$id":{"$t":"Histogram_Equalization_for_Facial_Features_Extraction"},"gsx$projecttitle":{"$t":"Histogram Equalization for Facial Features Extraction"},"gsx$tags":{"$t":"Image Processing, Computer Vision, Contrast Enhancement, Adaptive Histogram Equalization, C, MATLAB, Python, OpenCV"},"gsx$desc":{"$t":"The project aimed at developing robust image processing pipeline for enhancing contrast and edges in the facial image as a precursor to facial feature extraction. The task was accomplished by implementing \u003cem\u003eContrast Limited Adaptive Histogram Equalization (CLAHE)\u003c/em\u003e. The image so obtained was noisy and to suppress the same, the CLAHE was followed by applying \u003cem\u003eBidirectional Weighted Median Filter\u003c/em\u003e. The Proof-of-Concept was tested using MATLAB and Python, and the code was later ported to VHDL for FPGA programming."},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/dcgjs"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Panorama__Image_Stitching"},"content":{"type":"text","$t":"projecttitle: Panorama: Image Stitching, tags: Image Processing, ORB Detector, Pattern Recognition, Python, OpenCV, desc: Used Oriented FAST and Rotated BRIEF for keypoint extraction in individual images and matched the descriptors using FlannBasedMatcher to stitch the images."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/dcgjs"}],"gsx$id":{"$t":"Panorama__Image_Stitching"},"gsx$projecttitle":{"$t":"Panorama: Image Stitching"},"gsx$tags":{"$t":"Image Processing, ORB Detector, Pattern Recognition, Python, OpenCV"},"gsx$desc":{"$t":"Used Oriented FAST and Rotated BRIEF for keypoint extraction in individual images and matched the descriptors using FlannBasedMatcher to stitch the images."},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/ddv49"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Image_Denoising_using_Deep_Learning_and_Artificial_Neural_Networks"},"content":{"type":"text","$t":"projecttitle: Image Denoising using Deep Learning and Artificial Neural Networks, tags: Computer Vision, Image Processing, Artificial Intelligence, MATLAB, desc: For a given dataset of images of scanned text, that has seen better days, removed noise using Artificial Neural Networks. Achieved decrease in error rate for OCR by about 60% as compared to unprocessed image."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/ddv49"}],"gsx$id":{"$t":"Image_Denoising_using_Deep_Learning_and_Artificial_Neural_Networks"},"gsx$projecttitle":{"$t":"Image Denoising using Deep Learning and Artificial Neural Networks"},"gsx$tags":{"$t":"Computer Vision, Image Processing, Artificial Intelligence, MATLAB"},"gsx$desc":{"$t":"For a given dataset of images of scanned text, that has seen better days, removed noise using Artificial Neural Networks. Achieved decrease in error rate for OCR by about 60% as compared to unprocessed image."},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/d415a"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Finger_Print_Recognition_using_BLPOC_Algorithm"},"content":{"type":"text","$t":"projecttitle: Finger Print Recognition using BLPOC Algorithm, tags: Image Processing, Pattern Recognition, Correlation, Python, MATLAB, OpenCV, desc: Implemented a Band Limited Phase Only Correlation (BLPOC) Fingerprint Matching Algorithm for Fingerprint Recognition."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/d415a"}],"gsx$id":{"$t":"Finger_Print_Recognition_using_BLPOC_Algorithm"},"gsx$projecttitle":{"$t":"Finger Print Recognition using BLPOC Algorithm"},"gsx$tags":{"$t":"Image Processing, Pattern Recognition, Correlation, Python, MATLAB, OpenCV"},"gsx$desc":{"$t":"Implemented a Band Limited Phase Only Correlation (BLPOC) Fingerprint Matching Algorithm for Fingerprint Recognition."},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/d5fpr"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Finger_Point_Detection_for_Human_Computer_Interaction"},"content":{"type":"text","$t":"projecttitle: Finger Point Detection for Human Computer Interaction, tags: Computer Vision, Image Processing, Microprocessor Programming, Hardware Interfacing, Arduino, OpenCV, Python, desc: Developed a low-cost module for finger point detection using OpenCV library for Human Computer Interaction. The module could be used to perform click and point operations on Projector’s Screen"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/d5fpr"}],"gsx$id":{"$t":"Finger_Point_Detection_for_Human_Computer_Interaction"},"gsx$projecttitle":{"$t":"Finger Point Detection for Human Computer Interaction"},"gsx$tags":{"$t":"Computer Vision, Image Processing, Microprocessor Programming, Hardware Interfacing, Arduino, OpenCV, Python"},"gsx$desc":{"$t":"Developed a low-cost module for finger point detection using OpenCV library for Human Computer Interaction. The module could be used to perform click and point operations on Projector’s Screen"},"gsx$link":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/d6ua4"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Deep_Learning_to_Clone_Human_Driving_Behavior"},"content":{"type":"text","$t":"projecttitle: Deep Learning to Clone Human Driving Behavior, tags: Deep Learning, Computer Vision, Transfer Learning, Tensorflow, Keras, Python, desc: Built and trained a convolutional neural network for end-to-end driving in a simulator, using TensorFlow and Keras. Used optimization techniques such as regularization and dropout to generalize the network for driving on multiple tracks., link: https://github.com/sumitbinnani/CarND-Behavioral-Cloning-P3"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/d6ua4"}],"gsx$id":{"$t":"Deep_Learning_to_Clone_Human_Driving_Behavior"},"gsx$projecttitle":{"$t":"Deep Learning to Clone Human Driving Behavior"},"gsx$tags":{"$t":"Deep Learning, Computer Vision, Transfer Learning, Tensorflow, Keras, Python"},"gsx$desc":{"$t":"Built and trained a convolutional neural network for end-to-end driving in a simulator, using TensorFlow and Keras. Used optimization techniques such as regularization and dropout to generalize the network for driving on multiple tracks."},"gsx$link":{"$t":"https://github.com/sumitbinnani/CarND-Behavioral-Cloning-P3"}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/d88ul"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Kalman_Filter_for_Pedestrian_Motion_Tracking"},"content":{"type":"text","$t":"projecttitle: Kalman Filter for Pedestrian Motion Tracking, tags: Artificial Intelligence, Extended Kalman Filter, Unscented Kalman Filter, Motion Tracking, desc: Implemented a Extended as well as Unscented Kalman Filter algorithm in C++ capable of tracking a pedestrian's motion in two dimensions, link: https://github.com/sumitbinnani/Unscented-Kalman-Filter-Project"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/d88ul"}],"gsx$id":{"$t":"Kalman_Filter_for_Pedestrian_Motion_Tracking"},"gsx$projecttitle":{"$t":"Kalman Filter for Pedestrian Motion Tracking"},"gsx$tags":{"$t":"Artificial Intelligence, Extended Kalman Filter, Unscented Kalman Filter, Motion Tracking"},"gsx$desc":{"$t":"Implemented a Extended as well as Unscented Kalman Filter algorithm in C++ capable of tracking a pedestrian's motion in two dimensions"},"gsx$link":{"$t":"https://github.com/sumitbinnani/Unscented-Kalman-Filter-Project"}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/dkvya"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Sign_Language_Recognizer"},"content":{"type":"text","$t":"projecttitle: Sign Language Recognizer, tags: Artificial Intelligence, Hidden Markov Models (HMMs), Bayesian Information Creterion, Deviance Information Creterion, Sign Langauage, Python, desc: Built a system that can recognize words communicated using the American Sign Language (ASL). Trained a set of Hidden Markov Models (HMMs) using part of a preprocessed dataset of tracked hand and nose positions extracted from video to try and identify individual words from test sequences. Experimented with model selection techniques including BIC, DIC, and K-fold Cross Validation., link: https://github.com/sumitbinnani/Sign-Language-Recognizer"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/dkvya"}],"gsx$id":{"$t":"Sign_Language_Recognizer"},"gsx$projecttitle":{"$t":"Sign Language Recognizer"},"gsx$tags":{"$t":"Artificial Intelligence, Hidden Markov Models (HMMs), Bayesian Information Creterion, Deviance Information Creterion, Sign Langauage, Python"},"gsx$desc":{"$t":"Built a system that can recognize words communicated using the American Sign Language (ASL). Trained a set of Hidden Markov Models (HMMs) using part of a preprocessed dataset of tracked hand and nose positions extracted from video to try and identify individual words from test sequences. Experimented with model selection techniques including BIC, DIC, and K-fold Cross Validation."},"gsx$link":{"$t":"https://github.com/sumitbinnani/Sign-Language-Recognizer"}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/dmair"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Artificial_Intelligence_for_Game_Playing_Agent"},"content":{"type":"text","$t":"projecttitle: Artificial Intelligence for Game Playing Agent, tags: Artificial Intelligence, Minimax, Alpha-beta Pruning, Iterative Deepening, Python, desc: Created an AI that beats human opponents in the game of Isolation using Minimax, Alpha-Beta Search, and Iterative Deepening., link: https://github.com/sumitbinnani/AIND-Isolation"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/dmair"}],"gsx$id":{"$t":"Artificial_Intelligence_for_Game_Playing_Agent"},"gsx$projecttitle":{"$t":"Artificial Intelligence for Game Playing Agent"},"gsx$tags":{"$t":"Artificial Intelligence, Minimax, Alpha-beta Pruning, Iterative Deepening, Python"},"gsx$desc":{"$t":"Created an AI that beats human opponents in the game of Isolation using Minimax, Alpha-Beta Search, and Iterative Deepening."},"gsx$link":{"$t":"https://github.com/sumitbinnani/AIND-Isolation"}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/dnp34"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Air_Cargo_Transport_Planning"},"content":{"type":"text","$t":"projecttitle: Air Cargo Transport Planning, tags: Artificial Intelligence, A* Heuristics, Planning, Python, desc: Used logic and planning techniques to create an AI that finds the most efficient route to route cargo around the world to their respective destinations. This project used a combination of propositional logic and search along with A* heuristics to find optimal planning solutions., link: https://github.com/sumitbinnani/AIND-Planning"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/dnp34"}],"gsx$id":{"$t":"Air_Cargo_Transport_Planning"},"gsx$projecttitle":{"$t":"Air Cargo Transport Planning"},"gsx$tags":{"$t":"Artificial Intelligence, A* Heuristics, Planning, Python"},"gsx$desc":{"$t":"Used logic and planning techniques to create an AI that finds the most efficient route to route cargo around the world to their respective destinations. This project used a combination of propositional logic and search along with A* heuristics to find optimal planning solutions."},"gsx$link":{"$t":"https://github.com/sumitbinnani/AIND-Planning"}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/dp3nl"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Vehicle_Detection"},"content":{"type":"text","$t":"projecttitle: Vehicle Detection, tags: Computer Vision, Image Processing, HOG, SVM, OpenCV, Python, scikit-learn, desc: Created a vehicle detection and tracking pipeline with OpenCV, histogram of oriented gradients (HOG), and support vector machines (SVM). Optimized and evaluated the model on video data from a automotive camera taken during highway driving., link: https://github.com/sumitbinnani/CarND-Vehicle-Detection"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/dp3nl"}],"gsx$id":{"$t":"Vehicle_Detection"},"gsx$projecttitle":{"$t":"Vehicle Detection"},"gsx$tags":{"$t":"Computer Vision, Image Processing, HOG, SVM, OpenCV, Python, scikit-learn"},"gsx$desc":{"$t":"Created a vehicle detection and tracking pipeline with OpenCV, histogram of oriented gradients (HOG), and support vector machines (SVM). Optimized and evaluated the model on video data from a automotive camera taken during highway driving."},"gsx$link":{"$t":"https://github.com/sumitbinnani/CarND-Vehicle-Detection"}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/df9om"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Lane_Finding"},"content":{"type":"text","$t":"projecttitle: Lane Finding, tags: Computer Vision, Image Processing, OpenCV, Python, desc: Built an advanced lane-finding algorithm using distortion correction, image rectification, color transforms, and gradient thresholding. Identified lane curvature and vehicle displacement. Overcame environmental challenges such as shadows and pavement changes., link: https://github.com/sumitbinnani/CarND-Advanced-Lane-Lines"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/df9om"}],"gsx$id":{"$t":"Lane_Finding"},"gsx$projecttitle":{"$t":"Lane Finding"},"gsx$tags":{"$t":"Computer Vision, Image Processing, OpenCV, Python"},"gsx$desc":{"$t":"Built an advanced lane-finding algorithm using distortion correction, image rectification, color transforms, and gradient thresholding. Identified lane curvature and vehicle displacement. Overcame environmental challenges such as shadows and pavement changes."},"gsx$link":{"$t":"https://github.com/sumitbinnani/CarND-Advanced-Lane-Lines"}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/dgo93"},"updated":{"$t":"2017-06-16T16:04:27.841Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Traffic_Sign_Classification"},"content":{"type":"text","$t":"projecttitle: Traffic Sign Classification, tags: Deep Learning, Transfer Learning, Tensorflow, Keras, Python, desc: Built and trained a deep neural network to classify traffic signs, using TensorFlow. Experimented with different network architectures. Performed image pre-processing and validation to guard against overfitting., link: https://github.com/sumitbinnani/CarND-Traffic-Sign-Classifier-Project"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/10peBE12ztcAVdTurq8fewJLJMRZVwHgQHMnC6E64qgI/1/public/values/dgo93"}],"gsx$id":{"$t":"Traffic_Sign_Classification"},"gsx$projecttitle":{"$t":"Traffic Sign Classification"},"gsx$tags":{"$t":"Deep Learning, Transfer Learning, Tensorflow, Keras, Python"},"gsx$desc":{"$t":"Built and trained a deep neural network to classify traffic signs, using TensorFlow. Experimented with different network architectures. Performed image pre-processing and validation to guard against overfitting."},"gsx$link":{"$t":"https://github.com/sumitbinnani/CarND-Traffic-Sign-Classifier-Project"}}]}}